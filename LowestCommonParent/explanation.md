A naive approach for this problem would be to just travel the tree of files from top to bottom (that is, from the root to the "leaf nodes") until we find the desired files and then conclude the least commont parent by looking at the path we went through to get to each file.

However, it seems more natural to travel through the files from bottom to top (that is, starting from the files we are asked to find their common parent to the desired common parent) because that way we avoid dealing with non relevant files that are not found in neither path of the files. So the general strategy will be to start with the given files and "go up until we find a common parent". We will do that efficiently in a way that the first common parent we find will be the nearest common parent.

First, we need to ensure we have some sense of "location" regarding the whole tree of files so we don't get lost in a search that will never find a common parent. For instance, if we want the common parent of a file A and its child B, going up one file at a time from A and from B at the same pace will never leave us in a common ancestor because the first file will always be the parent of the second one. For that, we introduce the notion of "depth": In a tree, the root has depth 0 and, for all the other nodes, their depth will be the depth of their parent plus one. This is pretty easy to compute because all we have to do is set the depth of the root as 0 and include the instruction that sets "depth of B" as "depth of A + 1" when we call A->addChild(B). This concept will give us a notion of "levels" within the filesystem.

This will help us in our algorithm because if we have two files of different depths, we now know that we have to first equalize their depths before starting the search. That's because the number "A->depth - B->depth" is invariant when we replace A and B by their parents, and since two files with different depths cannot be equal, this strategy will never give us two equal files.

So the first step of our algorithm will be to check if the two given files have the same depth and, if not, take the lowest file up until we have two files on the same level. When we have two files on the same level, we can be sure that the desired first common parent will be the first file we find equal when traveling up from A and B at the same pace. So the second step of the algorithm will be to just go up until we find a common parent.

However, if we do these two steps greedily, the resulting algorithm will be quite inefficient. We need a way to perform the action "go up until some property X is satisfied" efficiently. The main issue with going up one file at a time is that its worst-case time complexity is linear. You could have a situation where the desired file is 10000 levels up, and doing 10000 iterations will result in quite a waste of time and resources.

To shortcut this, we will use a technique called "sparse table". For each file we will store an array of pointers to other files such that the i-th pointer is the parent of that file going 2^i levels up. We will call this array "ancestors". That way, we will be able to jump several levels at a time when going up. For instance, if we are at a file with depth 1026 and want to go to its parent at level 2, instead of going up one level 1024 times, we can go to the 11th element of the sparse table, which will be stored in file->ancestors[10], which means we will go up 2^10 = 1024 levels in one iteration. Note that, in each step we can halve the distance we want to jump since we can jump any distance power of 2 and, for any distance d, there's a power of 2 between d/2 and d. That way, going up to a certain depth will take at most O(logn) iterations instead of the previous O(n) iterations (where n is the distance we want to jump, which is upper bounded by the number of files).

We can do the same when looking for the first common parent of two files: Let's assume A and B have the same depth and their first common parent C is 14 levels up. Then, this means that their parent 8 levels up is different, so A->ancestors[3] and B->ancestors[3] are different. We can also say that their parent 16 levels up is the same (because it is the parent 2 levels up from C), so A->ancestors[4] and B->ancestors[4] will be equal. So we can directly jump 8 levels at the same time because we know that the first common parent of A->ancestor[3] and B->ancestors[3] will be C as well. So, in each step, we can halve the distance between the files and their first common parent by jumping to the last pointer in the "ancestors" array that is not shared between the two files.
Also note that the index of this last not-shared pointer can only decrease through iterations because if C is d levels above A and B, then we will jump 2^i where i es the greatest number such that 2^i <= d. This means that d/2 < 2^i and therefore d-2^i < d/2.
Thus, we can find the first common parent in O(logn) iterations instead of the previous O(n).

On the other hand, we have to preprocess each file when adding it to the filesystem. We have to calculate its depth (which is an O(1) computation) and fill its sparse table "ancestors" which will take O(logn) since we can get each ancestor in O(1) using the previous ancestor (the 2^i-th parent of a node is the 2^(i-1)-th parent of its 2^(i-1)-th parent). This will result in a filesystem that takes O(logn) in time and space complexity to add a new file and O(logn) time complexity to find the first common parent of each node.
